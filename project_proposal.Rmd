---
title: 'DS 2020 - Project Proposal'
author: "Eitan Tuchin, Tirmidi Mohamed"
date: "03/27/2025"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Proposal

### Research Topic

The topic for our research project is about bad-drivers in the US.

### Team Members

The team members working on this project are Eitan Tuchin and Tirmidi Mohamed.

### Description of Dataset

```{r}
data <- read.csv("https://raw.githubusercontent.com/fivethirtyeight/data/master/bad-drivers/bad-drivers.csv")
head(data)
summary(data)
```
Our dataset contains varying statistics relating to specifically bad-drivers throughout the United States including the District of Columbia. For each state (row) there are 7 columns that contain statistics like percentage of drivers involved in fatal collisions who were impaired by alcohol use. Each metric is numerical, either being an int or a float. Most deal with the "number of" or "percentage of" drivers in that state.

### First Data Cleaning Steps

```{r}
# check for missing values
missing_values <- colSums(is.na(data))
print("Missing values per column:")
print(missing_values)

# rename columns for simplicity
colnames(data) <- c("State", "Fatal_Collisions_Per_Billion", "Percent_Speeding", 
                    "Percent_Alcohol", "Percent_Not_Distracted", "Percent_No_Prior", 
                    "Insurance_Premiums", "Losses_Per_Driver")

str(data)

# convert State to factor 
data$State <- as.factor(data$State)
numeric_cols <- c("Fatal_Collisions_Per_Billion", "Percent_Speeding", "Percent_Alcohol", 
                  "Percent_Not_Distracted", "Percent_No_Prior", "Insurance_Premiums", 
                  "Losses_Per_Driver")
data[numeric_cols] <- lapply(data[numeric_cols], as.numeric)

# check for duplicate rows
duplicates <- duplicated(data)
print("Number of duplicate rows:")
print(sum(duplicates))

head(data)
summary(data)

```

### Marginal Summaries

```{r}
# convert numeric columns to numeric type (just in case)
numeric_cols <- c("Fatal_Collisions_Per_Billion", "Percent_Speeding", "Percent_Alcohol", 
                  "Percent_Not_Distracted", "Percent_No_Prior", "Insurance_Premiums", 
                  "Losses_Per_Driver")
data[numeric_cols] <- lapply(data[numeric_cols], as.numeric)

print("Basic Marginal Summaries:")
lapply(data[numeric_cols], summary)

marginal_summaries <- data.frame(
  Variable = numeric_cols,
  Mean = sapply(data[numeric_cols], mean, na.rm = TRUE),
  Median = sapply(data[numeric_cols], median, na.rm = TRUE),
  Std_Dev = sapply(data[numeric_cols], sd, na.rm = TRUE),
  Min = sapply(data[numeric_cols], min, na.rm = TRUE),
  Max = sapply(data[numeric_cols], max, na.rm = TRUE)
)

# round the results for readability
marginal_summaries[, 2:6] <- round(marginal_summaries[, 2:6], 2)

print("Custom Marginal Summaries:")
print(marginal_summaries)
```

### Project Idea (Questions we Are Answering)

We have three questions we are trying to answer: 
1: Can we identify regional patterns (e.g., Northeast, South, Midwest, West) in bad driving behaviors like speeding and alcohol impairment, and how do these align with fatal collision rates? 
2: Are there states where the percentage of speeding-related fatal collisions significantly deviates from the national average, and what state-specific factors (e.g., speed limits, road conditions) might explain this? 
3: How well can a predictive model (e.g., linear regression) use percentages of speeding, alcohol impairment, and distraction to estimate fatal collisions per billion miles, and which factor is most influential?