---
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Analysis on Bad Drivers in the United States
#### Eitan Tuchin, Tirmidi Mohamed

## Introduction
The goal of our project is to explore data about bad drivers in the United States so that we can better understand causes of fatal car crashes and point out common characteristics about bad drivers. Since driving is such a common activity that we as humans do on a day-to-day basis it becomes imperative to analyze times at when driving has gone wrong in the name of trying to reduce the risk for these dangerous situations. We hope that our exploration and the analysis of the data we looked at will lead to more effective driving policies throughout America and an altogether smaller fatalities reported in later data.

To achieve our project's goal we will answer the following questions:

  1. Can we identify regional patterns in bad driving behaviors like speeding and alcohol impairment, and how do          these align with fatal collision rates? 

  2. Are there states where the percentage of speeding-related fatal collisions significantly deviates from the           national average, and what state-specific factors might explain this? 

  3. How well can a predictive model use percentages of speeding, alcohol impairment, and distraction to                  estimate fatal collisions per billion miles, and which factor is most influential?
  
These will be the main questions that we try and answer throughout our work on this project. We hope to get to some meaningful conclusions about bad drivers and their common trends in the United States.

## Data
### Structure

The link to our dataset is https://github.com/fivethirtyeight/data/blob/master/bad-drivers/bad-drivers.csv. The github link points to a csv file under a repository. Our dataset contains varying statistics relating to specifically bad-drivers throughout the United States including the District of Columbia. For each state (row) there are 7 columns that contain statistics like percentage of drivers involved in fatal collisions who were impaired by alcohol use. Each metric is numerical, either being an int or a float. Most deal with the "number of" or "percentage of" drivers in that state.

Overall our dataset is pretty simple and it contains all of the data we need for answering our questions. 

### Cleaning

The first thing we need to do is read in the csv file and take a look at it:

```{r}
data <- read.csv("bad-drivers.csv")
summary(data)
head(data)
```

The first thing we did cleaning wise was check for missing values in our data by iterating over every column in our data and checking if the value was NA or not. Then we removed any columns with missing values.

```{r}
missing_values <- colSums(is.na(data))
print("Missing values per column:")
print(missing_values)
data <- data[, colSums(is.na(data)) == 0]  
print("Columns after removing those with NAs:")
print(names(data))
```

Then, for simplicity sake, we wanted to rename some of the columns in our data because they weren't as data explorable and accessible friendly. For example, we have a column named "Number of drivers involved in fatal collisions per billion miles" which would be annoying to access since it's separated by white-space and very long. We also checked to see if we got the intended result.

```{r}
colnames(data) <- c("State", "Fatal_Collisions_Per_Billion", "Percent_Speeding", 
                    "Percent_Alcohol", "Percent_Not_Distracted", "Percent_No_Prior", 
                    "Insurance_Premiums", "Losses_Per_Driver")
str(data)
```

Since we are going to be doing some statistical modeling later to answer our question we decided to turn the State column into a factor type from a String. Then we grabbed all the columns that contained numeric data and stored them. We then ensured that these specified columns are treated as numbers and not anything else. Now we have easy data to work with.

```{r}
data$State <- as.factor(data$State)
numeric_cols <- c("Fatal_Collisions_Per_Billion", "Percent_Speeding", "Percent_Alcohol", 
                  "Percent_Not_Distracted", "Percent_No_Prior", "Insurance_Premiums", 
                  "Losses_Per_Driver")
data[numeric_cols] <- lapply(data[numeric_cols], as.numeric)
```

Lastly, we need to check if we have any duplicate rows in our data. Then we removed those rows from the data.

```{r}
duplicates <- duplicated(data)
print("Number of duplicate rows:")
print(sum(duplicates))

data <- data[!duplicates, ]  

print("Final cleaned data:")
head(data)
```

After cleaning our data it's ready to be analyzed and interpreted.

### Variables

Here we list all of the variables (columns) in our dataset:

* State - The state to which the statistics in the row are attributed to.
* Fatal_Collisions_Per_Billion - Number of drivers involved in fatal collisions per billion miles.
* Percent_Speeding - Percentage of drivers who were involved in fatal collisions who were speeding.
* Percent_Alcohol - Percentage of drivers who were involved in fatal collisions who were alcohol-impaired.
* Percent_Not_Distracted - Percentage of drivers who were involved in fatal collisions who were not distracted.
* Percent_No_Prior - Percentage of drivers who were involved in fatal collisions who had not been in any previous accidents
* Insurance_Premiums - Average cost of car insurance premiums in that state.
* Losses_Per_Driver - Losses incurred by insurance companies for collisions per insured driver.

## Results

```{r}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(tidyr)
library(reshape2)
```


### Can we identify regional patterns in bad driving behaviors like speeding and alcohol impairment, and how do these     align with fatal collision rates? 

```{r}
divisions <- list(
  `New England` = c("Connecticut", "Maine", "Massachusetts", "New Hampshire", "Rhode Island", "Vermont"),
  `Middle Atlantic` = c("New Jersey", "New York", "Pennsylvania"),
  `East North Central` = c("Illinois", "Indiana", "Michigan", "Ohio", "Wisconsin"),
  `West North Central` = c("Iowa", "Kansas", "Minnesota", "Missouri", "Nebraska", "North Dakota", "South Dakota"),
  `South Atlantic` = c("Delaware", "Florida", "Georgia", "Maryland", "North Carolina", "South Carolina", 
                      "Virginia", "West Virginia", "District of Columbia"),
  `East South Central` = c("Alabama", "Kentucky", "Mississippi", "Tennessee"),
  `West South Central` = c("Arkansas", "Louisiana", "Oklahoma", "Texas"),
  `Mountain` = c("Arizona", "Colorado", "Idaho", "Montana", "Nevada", "New Mexico", "Utah", "Wyoming"),
  `Pacific` = c("Alaska", "California", "Hawaii", "Oregon", "Washington")
)

data <- data %>%
  mutate(Division = case_when(
    State %in% divisions$`New England` ~ "New England",
    State %in% divisions$`Middle Atlantic` ~ "Middle Atlantic",
    State %in% divisions$`East North Central` ~ "East North Central",
    State %in% divisions$`West North Central` ~ "West North Central",
    State %in% divisions$`South Atlantic` ~ "South Atlantic",
    State %in% divisions$`East South Central` ~ "East South Central",
    State %in% divisions$`West South Central` ~ "West South Central",
    State %in% divisions$`Mountain` ~ "Mountain",
    State %in% divisions$`Pacific` ~ "Pacific",
    TRUE ~ NA_character_
  ))

division_summary <- data %>%
  group_by(Division) %>%
  summarise(
    Avg_Percent_Speeding = mean(Percent_Speeding, na.rm = TRUE),
    Avg_Percent_Alcohol = mean(Percent_Alcohol, na.rm = TRUE),
    Avg_Fatal_Collisions = mean(Fatal_Collisions_Per_Billion, na.rm = TRUE)
  )

print("Division Summary:")
print(division_summary)

# Reshape data for heatmap
heatmap_data <- division_summary %>%
  select(Division, Avg_Percent_Speeding, Avg_Percent_Alcohol, Avg_Fatal_Collisions) %>%
  melt(id.vars = "Division")

# Plot
ggplot(heatmap_data, aes(x = variable, y = Division, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightyellow", high = "red", name = "Average Value") +
  labs(
    title = "Regional Patterns: Speeding, Alcohol, and Fatal Collisions",
    subtitle = "Darker shades indicate higher values",
    x = "Metric",
    y = "Region"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


ggplot(data, aes(x = Percent_Alcohol, y = Percent_Speeding, 
                 size = Fatal_Collisions_Per_Billion, color = Division)) +
  geom_point(alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, color = "black", linetype = "dashed") +
  scale_size_continuous(name = "Fatal Collisions\n(per billion miles)") +
  labs(
    title = "Alcohol vs. Speeding in Fatal Collisions",
    subtitle = "Point size = Fatal collision rate; Color = Region",
    x = "Alcohol-Impaired Drivers (%)",
    y = "Speeding Drivers (%)"
  ) +
  theme_minimal()
```

### Are there states where the percentage of speeding-related fatal collisions significantly deviates from the           national average, and what state-specific factors (e.g., speed limits, road conditions) might explain this? 

### How well can a predictive model (e.g., linear regression) use percentages of speeding, alcohol impairment, and       distraction to estimate fatal collisions per billion miles, and which factor is most influential?


## Conclusion

